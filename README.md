# ðŸ§  RAG Complaint Chatbot

**Intelligent Complaint Analysis for Financial Services**

This repository contains the implementation of a **Retrieval-Augmented Generation (RAG)** system built using **CFPB consumer complaint data**.
The project transforms raw, unstructured customer complaints into **searchable, evidence-backed insights** for internal teams such as Product, Support, and Compliance.

This work is part of **10 Academy â€“ AI Mastery (Week 7 Challenge)**.

---

## ðŸ“Œ Project Overview

CrediTrust Financial receives thousands of complaints each month across multiple financial products.
Manual analysis is slow, inconsistent, and does not scale.

This project addresses that challenge by:

* Enabling **semantic search** over complaint narratives
* Retrieving the most relevant complaint excerpts using vector similarity
* Generating **grounded answers** with an LLM using retrieved evidence
* Displaying **sources** to increase transparency and trust

---

## ðŸ—‚ Repository Structure

```
rag-complaint-chatbot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ vector_store/                # Persisted FAISS index
â”œâ”€â”€ notebooks/                   # EDA & experimentation
â”‚   â”œâ”€â”€ eda_preprocessing.ipynb
â”‚   â”œâ”€â”€ text_chunking_embedding.ipynb
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ src/                         # Modular production code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ text_cleaning.py
â”‚   â”œâ”€â”€ chunking.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â””â”€â”€ prompt_templates.py
â”œâ”€â”€ tests/                       # Unit tests
â”‚   â”œâ”€â”€ test_chunking.py
â”‚   â”œâ”€â”€ test_embeddings.py
â”‚   â””â”€â”€ test_retriever.py
â”œâ”€â”€ app.py                       # Gradio / Streamlit interface
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

> ðŸ” **Note**: Initial development was notebook-driven for exploration and learning.
> Core logic has been (or is being) **extracted into the `src/` package** to improve maintainability, testability, and production readiness.

---

## âœ… Task 1: Exploratory Data Analysis (EDA) & Preprocessing

### ðŸŽ¯ Objective

Understand the structure, distribution, and quality of consumer complaint narratives and prepare clean text for downstream NLP tasks.

### ðŸ” Key Steps

* Loaded **82,164 complaints** with complete narratives
* Analyzed distribution across financial products
* Analyzed narrative length:

  * Majority are **short (0â€“250 words)**
  * A small number are **very long (up to ~6,400 words)**
* Visualized narrative length distribution using a histogram
  â†’ Strong right-skewed distribution

### ðŸ§¹ Text Cleaning

* Converted text to lowercase
* Removed special characters and boilerplate phrases
* Filtered complaints to include:

  * Credit Cards
  * Personal Loans
  * Savings Accounts
  * Money Transfers

### ðŸ“¦ Deliverables

* `data/processed/filtered_complaints.csv`
* EDA notebook with visualizations
* Written summary of findings

---

## âœ… Task 2: Text Chunking, Embedding & Vector Store Indexing

### ðŸŽ¯ Objective

Prepare complaint narratives for **semantic search** by converting them into embeddings and indexing them in a vector database.

### ðŸ” Key Steps

### 1ï¸âƒ£ Sampling

* Stratified sample of **~10,000â€“15,000 complaints**
* Maintained proportional representation across product categories

### 2ï¸âƒ£ Chunking

* Split long narratives into overlapping chunks
* Overlap preserved context across chunk boundaries
* Logic later modularized into `src/chunking.py`

### 3ï¸âƒ£ Embedding

* Used **`sentence-transformers/all-MiniLM-L6-v2`**
* Chosen for speed, small size, and strong semantic performance
* Implemented in `src/embeddings.py`

### 4ï¸âƒ£ Vector Store Indexing

* Stored embeddings in **FAISS**
* Persisted index to disk
* Stored metadata with each chunk:

  * complaint_id
  * product_category
  * issue / sub-issue
* Metadata handling extracted into reusable functions

### ðŸ“¦ Deliverables

* Chunking & embedding notebook
* Persisted FAISS index in `vector_store/`
* Modular embedding and indexing logic in `src/`

---

## âœ… Task 3: RAG Core Logic & Evaluation

### ðŸŽ¯ Objective

Build and evaluate a **retrieval-augmented generation pipeline** using the full-scale pre-built vector store.

### ðŸ§  RAG Pipeline

1. Embed user query
2. Retrieve top-k relevant complaint chunks
3. Inject retrieved context into a structured prompt
4. Generate grounded answer using an LLM

> âš ï¸ **Note on Performance**
> Loading large models (e.g., `Mistral-7B-Instruct`) may take **10â€“15+ minutes** on CPU-only machines. This is expected behavior.

### ðŸ§© Prompt Engineering

Prompts instruct the model to:

* Act as a financial analyst
* Use **only provided context**
* Explicitly state when information is insufficient

### ðŸ§ª Evaluation

* 5â€“10 representative business questions
* Manual qualitative evaluation
* Scored relevance, grounding, and clarity
* Results documented in an evaluation table

### ðŸ“¦ Deliverables

* Modular RAG logic in `src/rag_pipeline.py`
* Evaluation table and analysis in final report

---

## âœ… Task 4: Interactive Chat Interface

### ðŸŽ¯ Objective

Provide a **simple, trustworthy UI** for non-technical users.

### ðŸ–¥ Features

* Question input box
* Ask / Submit button
* AI-generated answer
* **Source complaint excerpts displayed**
* Clear / Reset functionality
* Optional streaming responses

### ðŸ›  Tools

* Gradio or Streamlit

### ðŸ“¦ Deliverables

* `app.py`
* Screenshots / GIFs in final report

---

## ðŸ§ª Testing & Code Quality Improvements

Based on feedback, the project emphasizes:

* âœ… **Extracting notebook logic into `src/` modules**
* âœ… **Explicit metadata handling**
* âœ… **Unit tests for chunking, embeddings, and retrieval**
* âœ… **Cleaner separation between experimentation and production code**

Tests are included under the `tests/` directory to ensure:

* Chunk boundaries are respected
* Embedding dimensions are correct
* Retriever returns relevant results

---

## ðŸš€ Usage

```bash
pip install -r requirements.txt
python app.py
```

---

## ðŸ§¾ Summary

This project demonstrates a full **end-to-end RAG system**:

* **Task 1**: Data understanding and cleaning
* **Task 2**: Chunking, embedding, and vector indexing
* **Task 3**: Retrieval + generation with evaluation
* **Task 4**: Interactive, trust-aware UI

Incorporating reviewer feedback, the project evolves from **notebook-driven exploration** into a **modular, testable, production-oriented system**.

---
